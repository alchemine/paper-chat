{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "from paper_chat.utils.elasticsearch_manager import (\n",
    "    ElasticSearchManager,\n",
    ")\n",
    "\n",
    "es_manager = ElasticSearchManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1706.03762\"\n",
    "\n",
    "response = es_manager.search(\n",
    "    \"papers_metadata\",\n",
    "    body={\"query\": {\"match\": {\"arxiv_id\": arxiv_id}}},\n",
    ")\n",
    "hits = response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'papers_metadata',\n",
       "  '_id': 'heYdzJABGHAP0wz7AOid',\n",
       "  '_score': 0.6931471,\n",
       "  '_ignored': ['abstract.keyword', 'summary.keyword'],\n",
       "  '_source': {'arxiv_url': 'https://arxiv.org/pdf/1706.03762',\n",
       "   'summary': '### Information\\n1. Title: Attention Is All You Need (주의는 모든 것이다)\\n2. Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\\n3. arXiv: https://arxiv.org/pdf/1706.03762v7\\n\\n### Abstract\\n1. **Challenges**: 저자들이 직면한 도전 과제는 복잡한 순차 변환 모델의 병목 현상과 병렬화의 어려움이다.\\n2. **Existing works**: 이전 연구들은 순환 신경망이나 합성곱 신경망을 기반으로 한 복잡한 모델을 사용했다.\\n3. **Limitations**: 기존 모델은 병목 현상과 병렬화 어려움으로 인해 한계가 있었다.\\n4. **Motivation**: 저자들은 순환 및 합성곱을 배제하고 오로지 어텐션 메커니즘에 기반한 새로운 신경망 구조, Transformer를 제안했다.\\n5. **Proposed method**: Transformer는 입력과 출력 간의 전역 의존성을 그리기 위해 순환을 완전히 배제하고 어텐션 메커니즘에만 의존하는 모델이다.\\n6. **Contributions**: 실험 결과, Transformer 모델은 훈련 시간이 적게 걸리고 더 병렬화가 가능하며 품질이 우수하다. 또한, 다른 작업에도 잘 일반화되는 것을 보여주었다.\\n\\n### 1. Introduction\\n1. 순환 신경망 및 게이트 순환 신경망은 순차 모델링 및 변환 문제에서 최첨단 접근 방식으로 확립되었다.\\n2. 최근 연구에서는 순환 언어 모델 및 인코더-디코더 아키텍처의 경계를 넓히는 노력이 계속되고 있다.\\n3. 순환 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치에 따라 계산을 분해한다.\\n   \\n### 2. Related Works\\n1. 어텐션 메커니즘은 입력 또는 출력 시퀀스에서의 거리에 관계없이 종속성을 모델링할 수 있게 해주어 매력적인 시퀀스 모델링 및 변환 모델의 중요한 부분이 되었다.\\n2. 그러나 대부분의 경우 이러한 어텐션 메커니즘은 순환 네트워크와 함께 사용된다.\\n3. 본 연구에서는 순환을 배제하고 입력과 출력 간의 전역 의존성을 그리기 위해 어텐션 메커니즘에 완전히 의존하는 Transformer 모델을 제안한다.\\n\\n### 3. Methodology\\n1. Transformer는 인코더와 디코더 모두에 대해 쌓인 셀프 어텐션 및 포인트별 완전히 연결된 레이어를 사용하는 모델 아키텍처를 따른다.\\n2. 인코더는 N=6개의 동일한 레이어로 구성되어 있으며, 각 레이어에는 멀티헤드 셀프 어텐션 메커니즘과 단순한 위치별 완전히 연결된 피드포워드 네트워크가 있다.\\n3. 디코더는 인코더 레이어의 출력에 대한 멀티헤드 어텐션을 수행하는 세 번째 서브 레이어를 삽입한다.\\n\\n### 4. Experiments\\n1. Transformer 모델은 WMT 2014 영어-독일 번역 작업에서 28.4 BLEU를 달성하여 기존 최고 결과를 개선했다.\\n2. 또한, Transformer는 다른 작업에도 잘 일반화되어 영어 구성 분석에 성공적으로 적용되었다.\\n\\n### 5. Discussion\\n1. Transformer 모델은 순환이나 합성곱 레이어를 사용하는 아키텍처보다 훨씬 빠르게 훈련될 수 있음을 입증했다.\\n2. 영어-독일 및 영어-프랑스 번역 작업에서 Transformer는 새로운 최고 성능을 달성했으며, 이전 모델들보다 훨씬 적은 훈련 비용이 필요했다.\\n\\n### 6. Conclusion\\n1. Transformer는 어텐션에 완전히 기반한 첫 번째 시퀀스 변환 모델로, 순환 레이어를 멀티헤드 셀프 어텐션으로 대체했다.\\n2. 번역 작업에서 Transformer는 순환이나 합성곱 레이어를 사용하는 아키텍처보다 훨씬 빠르게 훈련되었고, 새로운 최고 성능을 달성했다.\\n3. 앞으로의 연구 방향으로는 Transformer를 텍스트 이외의 입력 및 출력 모드에 적용하고, 이미지, 오디오 및 비디오와 같은 대규모 입력 및 출력을 효율적으로 처리하기 위한 로컬, 제한된 어텐션 메커니즘을 조사할 계획이다.',\n",
       "   'title': 'Attention Is All You Need',\n",
       "   'authors': ['Ashish Vaswani',\n",
       "    'Noam Shazeer',\n",
       "    'Niki Parmar',\n",
       "    'Jakob Uszkoreit',\n",
       "    'Llion Jones',\n",
       "    'Aidan N. Gomez',\n",
       "    'Lukasz Kaiser',\n",
       "    'Illia Polosukhin'],\n",
       "   'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'published': '2017-06-12T17:57:34Z',\n",
       "   'arxiv_id': '1706.03762',\n",
       "   'citation_count': 'N/A'}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arxiv_url': 'https://arxiv.org/pdf/1706.03762',\n",
       "  'summary': '### Information\\n1. Title: Attention Is All You Need (주의는 모든 것이다)\\n2. Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\\n3. arXiv: https://arxiv.org/pdf/1706.03762v7\\n\\n### Abstract\\n1. **Challenges**: 저자들이 직면한 도전 과제는 복잡한 순차 변환 모델의 병목 현상과 병렬화의 어려움이다.\\n2. **Existing works**: 이전 연구들은 순환 신경망이나 합성곱 신경망을 기반으로 한 복잡한 모델을 사용했다.\\n3. **Limitations**: 기존 모델은 병목 현상과 병렬화 어려움으로 인해 한계가 있었다.\\n4. **Motivation**: 저자들은 순환 및 합성곱을 배제하고 오로지 어텐션 메커니즘에 기반한 새로운 신경망 구조, Transformer를 제안했다.\\n5. **Proposed method**: Transformer는 입력과 출력 간의 전역 의존성을 그리기 위해 순환을 완전히 배제하고 어텐션 메커니즘에만 의존하는 모델이다.\\n6. **Contributions**: 실험 결과, Transformer 모델은 훈련 시간이 적게 걸리고 더 병렬화가 가능하며 품질이 우수하다. 또한, 다른 작업에도 잘 일반화되는 것을 보여주었다.\\n\\n### 1. Introduction\\n1. 순환 신경망 및 게이트 순환 신경망은 순차 모델링 및 변환 문제에서 최첨단 접근 방식으로 확립되었다.\\n2. 최근 연구에서는 순환 언어 모델 및 인코더-디코더 아키텍처의 경계를 넓히는 노력이 계속되고 있다.\\n3. 순환 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치에 따라 계산을 분해한다.\\n   \\n### 2. Related Works\\n1. 어텐션 메커니즘은 입력 또는 출력 시퀀스에서의 거리에 관계없이 종속성을 모델링할 수 있게 해주어 매력적인 시퀀스 모델링 및 변환 모델의 중요한 부분이 되었다.\\n2. 그러나 대부분의 경우 이러한 어텐션 메커니즘은 순환 네트워크와 함께 사용된다.\\n3. 본 연구에서는 순환을 배제하고 입력과 출력 간의 전역 의존성을 그리기 위해 어텐션 메커니즘에 완전히 의존하는 Transformer 모델을 제안한다.\\n\\n### 3. Methodology\\n1. Transformer는 인코더와 디코더 모두에 대해 쌓인 셀프 어텐션 및 포인트별 완전히 연결된 레이어를 사용하는 모델 아키텍처를 따른다.\\n2. 인코더는 N=6개의 동일한 레이어로 구성되어 있으며, 각 레이어에는 멀티헤드 셀프 어텐션 메커니즘과 단순한 위치별 완전히 연결된 피드포워드 네트워크가 있다.\\n3. 디코더는 인코더 레이어의 출력에 대한 멀티헤드 어텐션을 수행하는 세 번째 서브 레이어를 삽입한다.\\n\\n### 4. Experiments\\n1. Transformer 모델은 WMT 2014 영어-독일 번역 작업에서 28.4 BLEU를 달성하여 기존 최고 결과를 개선했다.\\n2. 또한, Transformer는 다른 작업에도 잘 일반화되어 영어 구성 분석에 성공적으로 적용되었다.\\n\\n### 5. Discussion\\n1. Transformer 모델은 순환이나 합성곱 레이어를 사용하는 아키텍처보다 훨씬 빠르게 훈련될 수 있음을 입증했다.\\n2. 영어-독일 및 영어-프랑스 번역 작업에서 Transformer는 새로운 최고 성능을 달성했으며, 이전 모델들보다 훨씬 적은 훈련 비용이 필요했다.\\n\\n### 6. Conclusion\\n1. Transformer는 어텐션에 완전히 기반한 첫 번째 시퀀스 변환 모델로, 순환 레이어를 멀티헤드 셀프 어텐션으로 대체했다.\\n2. 번역 작업에서 Transformer는 순환이나 합성곱 레이어를 사용하는 아키텍처보다 훨씬 빠르게 훈련되었고, 새로운 최고 성능을 달성했다.\\n3. 앞으로의 연구 방향으로는 Transformer를 텍스트 이외의 입력 및 출력 모드에 적용하고, 이미지, 오디오 및 비디오와 같은 대규모 입력 및 출력을 효율적으로 처리하기 위한 로컬, 제한된 어텐션 메커니즘을 조사할 계획이다.',\n",
       "  'title': 'Attention Is All You Need',\n",
       "  'authors': ['Ashish Vaswani',\n",
       "   'Noam Shazeer',\n",
       "   'Niki Parmar',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Llion Jones',\n",
       "   'Aidan N. Gomez',\n",
       "   'Lukasz Kaiser',\n",
       "   'Illia Polosukhin'],\n",
       "  'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "  'published': '2017-06-12T17:57:34Z',\n",
       "  'arxiv_id': '1706.03762',\n",
       "  'citation_count': 'N/A'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcs = [hit[\"_source\"] for hit in hits]\n",
    "srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1705.03762\"\n",
    "\n",
    "response = es_manager.search(\n",
    "    \"papers_metadata\",\n",
    "    body={\"query\": {\"match\": {\"arxiv_id\": arxiv_id}}},\n",
    ")\n",
    "hits = response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-chat-9TtSrW0h-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
